# Leveraging Large Language Models for Automated Code Migration and Repository-Level Tasks — Part II: Supplement — Specialized Language Models for Code

![Header image: stylized code graphics with the article title and author’s byline]  
> **Note:** A header banner featuring the article title and a small illustration of code snippets and data flow diagrams.

The development of specialized language models for code processing tasks has become a significant area of focus in natural language processing. This article is a supplement for the main Part II [article](https://medium.com/gopenai/leveraging-large-language-models-for-automated-code-migration-and-repository-level-tasks-part-ii-6377e7a76c8e) and examines the key components and trends in this domain, covering datasets, model architectures, and pretraining objectives.

## Datasets

The effectiveness of language models depends on the quality and quantity of their training data. Several large-scale datasets have been developed for code models for pretraining or fine-tuning:

- **CodeSearchNet**  
  Contains 20 GB of code across six programming languages, featuring 2.3 million functions paired with natural language documentation. Enables models to learn the relationship between code and its description.

- **CodeParrot**  
  Derived from GitHub repositories via Google’s BigQuery, encompasses approximately 50 GB of code with about 22 million Python files, offering exposure to diverse coding styles.

- **The Stack**  
  Part of the BigCode Project, offers over 6 TB of permissively licensed source code covering 358 programming languages, providing an extensive, varied corpus.

- **The Stack v2**  
  Expansion of The Stack: contains 3 billion+ files across 600+ languages, sourced from the Software Heritage archive—the most comprehensive code dataset to date.

- **Commit histories**  
  Used in datasets like CommitPackFT for instruction tuning; captures real-world coding practices by mining Git commit messages and diffs.

- **Synthetic data generation**  
  e.g., Phi-1 uses ChatGPT to generate 1 billion tokens of synthetic Python tutorials and exercises, enabling targeted training data creation.

## Model Architectures

Specialized language models for code fall into three categories: encoder models, encoder–decoder models, and decoder models.

### Encoder Models

- **CuBERT** – BERT-style pretraining on a code corpus.  
- **CodeBERT** – RoBERTa-based; uses MLM and Replaced Token Detection on CodeSearchNet with text–code pairs.  
- **GraphCodeBERT** – Adds data-flow objectives (edge prediction, node alignment) for syntax and semantic structure.  
- **SynCoBERT** – Incorporates AST-based edge prediction and contrastive learning.  
- **Code-MVP** – Builds on GraphCodeBERT with type inference and cross-view contrastive objectives.

### Encoder–Decoder Models

- **PyMT5 & T5-code** – T5 architecture pretraining and multi-task fine-tuning on code corpora.  
- **PLBART** – BART-based; 655 GB Java/Python/NL data with Denoising Autoencoding.  
- **CodeT5** – Span corruption, identifier tagging, masked identifier prediction, text-to-code and code-to-text generation.  
- **CodeT5+** – Adds causal LM and text–code contrastive learning.  
- **AlphaCode** – Shallow encoder + deep decoder (up to 41 B params); uses MLM/CLM and multi-query attention.  
- **NatGen** – “Naturalization” objective: convert modified code back to original form.

### Decoder Models

- **Traditional CLM Models** – GPT-C, CodeGPT, PolyCoder, CodeGen, PyCodeGPT, PanGu-Coder, CodeGeeX (Causal LM).  
- **Fill-in-the-Middle Models** – InCoder, SantaCoder, StarCoder (FIM + CLM for bidirectional context).  
- **Small but Powerful** – Phi-1/1.5/2 achieve high HumanEval pass@1 scores with ~1–2 B parameters.  
- **Efficiency Innovations** – Multi-Query Attention (MQA), Rotary Position Embeddings (RoPE), FlashAttention.

## Pretraining Objectives

The choice of objective shapes a model’s strengths:

### Causal Language Modeling (CLM)

Predict next token given previous ones.  
```

P(x) = ∏*{i=1}^{n} P(x\_i | x*{\<i})

```

### Masked Language Modeling (MLM)

Mask random tokens and recover them.  
```

L\_MLM = E\[log P(m | x^)]

```

### Span Corruption

Replace spans with sentinel tokens and reconstruct them.  
```

L\_SC = E\[∑*{i=1}^{k} log P(x\_i | x^, x*{\<i})]

```

### Fill-in-the-Middle (FIM)

Complete code snippets given both left and right context.

### Contrastive Learning

Improve representations by contrasting different views (e.g., text vs. AST).

### Multi-task Pretraining

Alternate between objectives during training for versatility across tasks.

### Data Flow Objectives

Predict relationships between variables based on data-flow through the program (e.g., GraphCodeBERT).

The field of code language models continues to evolve, with ongoing research into novel objectives that better capture programming language properties and software development workflows.  

