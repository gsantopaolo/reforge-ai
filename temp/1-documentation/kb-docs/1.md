

# Leveraging Large Language Models for Automated Code Migration and Repository-Level Tasks — Part II: Deep Dive into LLM Architectures for Code Tasks

This report, Part II of our series, provides a comprehensive overview of the current state of language models for code—covering their evolution, architectures, training techniques, evaluation methods, and applications, as well as challenges and future directions in this rapidly advancing field.

> **Note:** If you missed Part I, you can access it [here](https://medium.com/gopenai/leveraging-large-language-models-for-automated-code-migration-and-repository-level-tasks-part-i).

![Models for Code Taxonomy Diagram](#)  
> **Note:** A diagram showing various code-focused language models organized by architecture type (encoder, decoder, encoder–decoder, unified, diffusion, etc.), illustrating which families (e.g., BERT, GPT, T5, UniLM, diffusion) correspond to which tasks.

## 1. Introduction

The field of code processing using language models has seen rapid advancement in recent years, mirroring progress in natural language processing (NLP). The evolution of code-processing models closely parallels that of NLP itself. The introduction of the Transformer architecture by Vaswani et al. in 2017 marked a paradigm shift, enabling:

- **Self-attention:** Captures long-range dependencies more effectively than RNNs.  
- **Parallelism:** Enables efficient training on large code corpora.

The emergence of models like OpenAI’s Codex and GitHub’s Copilot has opened new possibilities in code generation, program synthesis, and automated software development.

### 1.1 Integration of Code-Specific Features

Key aspects of code-processing models include:

- **Bridging NLP & SE:** Integrating compiler-level representations (ASTs, CFGs) and unit tests.  
- **Advanced objectives:** Instruction tuning, infilling (FIM), contrastive learning.  
- **Scaling laws:** Tailored to code data.

#### 1.1.1 AST, IR, Control Flow & Data Flow

Code analysis relies on four core concepts:

- **Abstract Syntax Tree (AST):** Hierarchical structure of code.  
- **Intermediate Representation (IR):** Language-agnostic code form used in compilation.  
- **Control Flow Graph (CFG):** Execution order of instructions.  
- **Data Flow Graph (DFG):** Tracks how data moves through the program.

Source code → AST → IR → (CFG & DFG) analyses.

![AST/IR/CFG/DFG Diagram](#)  
> **Note:** Illustration of a simple Python function parsed into an AST, converted to IR, and then analyzed to produce control flow and data flow graphs.

#### 1.1.2 Integration of Multiple Features

The most advanced models combine multiple representations.  
**Example: Code-MVP** integrates:

- Source code & docstrings  
- ASTs & CFGs  
- Transformed code via renaming, loop exchanges, dead-code injection  
- Trained with MLM, type-prediction, contrastive learning  

---

## 2. LLM Architectures for Code Tasks

LLM architectures for code can be broadly categorized:

### 2.1 Encoder Models

- Inspired by BERT; suited for code understanding.  
- **Features:**  
  1. Bidirectional self-attention  
  2. Masked Language Modeling (MLM)  
  3. `[CLS]` token for sequence tasks

Typical flow:
```text
[Token IDs + Segment IDs + Position IDs]
    ↓
[Transformer encoder layers]
    ↓
[MLM head or classification head]
````

### 2.2 Decoder Models

* Exemplified by GPT; ideal for code generation.
* **Features:**

  1. Causal self-attention (attends only to previous tokens)
  2. Autoregressive generation

### 2.3 Encoder–Decoder Models

* Sequence-to-sequence (e.g., T5, CodeT5, PLBART)
* **Features:**

  1. Separate encoder/decoder stacks
  2. Cross-attention in decoder
  3. Handles variable-length input/output

### 2.4 Unified & Diffusion Models

* **UniLM:** Flexible masking for both bidirectional & autoregressive objectives.
* **Diffusion (e.g., CodeFusion):** Iteratively denoises code via non-autoregressive steps.

---

## 3. Pretraining Objectives

* **Causal LM (CLM):** Next-token prediction (GPT, Codex).
* **Masked LM (MLM):** Predict masked tokens (BERT-style).
* **Fill-in-the-Middle (FIM):** Complete spans given left/right context (InCoder, StarCoder).
* **Span Corruption:** Reconstruct removed spans (T5, CodeT5).
* **Contrastive Learning:** Improve representations by contrasting different views (e.g., code vs. AST).

Data flow objectives (e.g., GraphCodeBERT) help models grasp code semantics and relationships.

---

## 4. Specialized Language Models for Code

Models specifically optimized for code tasks—see the full list of specialized architectures and their benchmarks in the original article.

---

## 5. Universal LLMs for Code

Universal models handle generation, translation, and bug fixing across languages:

1. **Scale:** From 6.7B (AutoCoder) to 34B (Code Llama) parameters.
2. **Context Window:** Up to 128K tokens in DeepSeek-Coder-V2.
3. **Multilingual Benchmarks:** HumanEval-X, MBPP, etc.
4. **Licensing:** Permissive releases (e.g., Code Llama).

**Trends & Challenges:**

* Rapid release cycles (e.g., CodeGeeX → CodeGeeX2).
* Compute/resource demands.
* Ensuring security and correctness.

---

## 6. Fine-tuning Techniques

LLMs are fine-tuned post-pretraining to optimize for specific code tasks:

### 6.1 Task-specific Fine-tuning

* Add task-specific head (e.g., classification or generation).
* Train on labeled dataset; use `[CLS]` representation for classification or final hidden state for generation.

### 6.2 Multi-task Instruction Fine-tuning

* Fine-tune on multiple tasks with natural language instructions (e.g., FLAN).

### 6.3 Efficient Fine-tuning Methods

* **Adding parameters:** Prefix Tuning, Prompt Tuning, Adapter Tuning
* **Selective updates:** BitFit
* **Re-parameterization:** LoRA, AdaLoRA, QLoRA
* **Hybrid:** MAM Adapter, UniPELT

### 6.4 Instruction Tuning & RL for Code

#### 6.4.1 Instruction Tuning

* Train on diverse instruction datasets to improve zero-/few-shot performance.

#### 6.4.2 Supervised Fine-Tuning (SFT)

* Train on high-quality input/output examples.

#### 6.4.3 Reinforcement Learning for Code Generation

* Optimize for code correctness and efficiency using reward signals (e.g., RLHF, CodeRL, PPOCoder).

### 6.5 Advanced Prompt Optimization

#### 6.5.1 TextGrad

* Uses “textual gradients” from LLM feedback to iteratively refine prompts.

#### 6.5.2 DSPy

* Modular, programmatic framework with an automatic compiler and “teleprompters” for prompt optimization.

---

## 7. Evaluation of Language Models for Code

### 7.1 Code Understanding

* Metrics: Accuracy, F1, Mean Reciprocal Rank (MRR).

### 7.2 Code-to-Text & Summarization

* Metrics: BLEU, ROUGE.

### 7.3 Code Generation

* **Pass\@k:** Probability that one of *k* samples passes unit tests.
* **CodeBLEU:** AST & data-flow aware similarity.

### 7.4 Text-to-Code & Retrieval

* Metrics: MRR, NDCG, execution accuracy.

### 7.5 Code-to-Code Tasks

* Metrics: Perplexity, exact match, edit distance, compilation success, fix rate.

### 7.6 Syntactic & Semantic Metrics

* Syntactic validity, static analysis (Pylint/ESLint), CodeBLEU.

### 7.7 Program Synthesis Datasets

* CONCODE, APPS, HumanEval, MBPP, DS-1000, MathQA-Python, GSM8K-Python.

### 7.8 Repository-Level Evaluation

* New benchmarks: RepoBench, SWE-bench.

---

## 8. Computational Requirements & Model Sizes

### 8.1 Model Storage

* \~4 bytes/parameter (32-bit).

### 8.2 Training Memory

* \~20 bytes/parameter for optimizer states & activations.

### 8.3 Computational Complexity

* O(n²·d) for sequence length *n* and hidden dimension *d*.

These constraints drive techniques like model parallelism, gradient checkpointing, and precision reduction.

---

## 8.4 Scaling Laws & Data Requirements

Training LLMs involves trade-offs between:

* **Model scale (N):** Number of parameters.
* **Data size (D):** Volume of training examples.
* **Compute budget (C):** Total FLOPs available.

Performance follows a power-law:

```text
Performance ∝ (1/X)^(–α)
```

where X ∈ {N, D, C} and α ≈ 0.05–0.1.

![Scaling Laws Graphs](#)

> **Note:** Three-panel figure showing how increasing compute budget, dataset size, and model parameters each reduces test loss, following power-law relationships.

![Optimal Scaling Strategies](#)

> **Note:** Two-panel figure illustrating optimal compute vs. model scale (left) and optimal data size vs. model scale (right).

---

## 8.5 Efficient Training Strategies

* Mixed precision (e.g., 16-bit)
* Gradient accumulation for large batch simulation
* Model parallelism across GPUs
* Sparse attention approximations

---

## 9. LLMs in Software Development: Advancements, Applications & Future Challenges

### 9.1 Extended with Coding Tools

* **Interpreters:** PAL, PoT, ViperGPT
* **Unit tests:** CodeT, TiCoder
* **Self-debugging:** ChatGPT Interpreter, Open Interpreter
* **Planning & multi-agent:** CodePlan, MetaGPT, AutoGen

### 9.2 Integrated into IDEs & Frameworks

* **Code completion:** Copilot, CodeFuse, StarShip CodeGen
* **Coding assistants:** Devin (autonomous AI engineer)
* **Frameworks:** LangChain, AutoGPT, WorkGPT, MetaGPT

**Challenges & Future Directions:**

* Real-time performance for large models
* Code quality, security, and ethics
* Data contamination and model explainability
* Full-lifecycle code LLM ecosystems

---

# References

* Ahmad, W. U., Chakraborty, S., Ray, B., & Chang, K. W. (2021). Unified pre-training for program understanding and generation. *NAACL-HLT*.
* Bui, N. D., Yu, Y., & Jiang, L. (2021). InferCode: Self-supervised learning of code representations by predicting subtrees. *ICSE 2021*.
* Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., … & Zaremba, W. (2021). Evaluating large language models trained on code. *arXiv:2107.03374*.
* Chen, Z., Kommrusch, S. J., Tufano, M., Pouchet, L. N., Poshyvanyk, D., & Monperrus, M. (2018). Sequencer: Sequence-to-sequence learning for end-to-end program repair. *IEEE TSE*.
* Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., … & Zhou, M. (2020). CodeBERT: A pre-trained model for programming and natural languages. *EMNLP Findings*.
* Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., … & Zettlemoyer, L. (2023). InCoder: A generative model for code infilling and synthesis. *arXiv:2305.00350*.
* Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., … & Zhou, M. (2021). GraphCodeBERT: Pre-training code representations with data flow. *ICLR*.
* Jiang, Y., Gu, Q., Lu, H., Cai, W., Zhang, Z., & Su, Z. (2021). TreeBERT: A tree-based pre-trained model for programming language. *ICSME*.
* Kanade, A., Maniatis, P., Balakrishnan, G., & Shi, K. (2020). Learning and evaluating contextual embedding of source code. *ICML*.
* Kulal, S., Pasupat, P., Chandra, K., Lee, M., Padon, O., Aiken, A., & Liang, P. S. (2019). SPoC: Search-based pseudocode to code. *NeurIPS*.
* Li, R., Zhu, B., Yang, Z., Wang, W., & Zhang, Y. (2023). StarCoder: May the source be with you! *arXiv:2305.06161*.
* Li, Y., Wang, S., Nguyen, T. N., & Van Nguyen, S. (2022). A large-scale study of evaluating and improving the consistency of large language models for code generation. *arXiv:2210.13920*.
* Liu, F., Li, G., Zhao, Y., & Jin, Z. (2020). Multi-task learning based pre-trained language model for code completion. *ASE*.
* Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., … & Zhou, M. (2020). CodeBLEU: a method for automatic evaluation of code synthesis. *arXiv:2009.10297*.
* Rozière, B., Lachaux, M. A., Chanussot, L., & Lample, G. (2020). Unsupervised translation of programming languages. *NeurIPS*.
* Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. *NeurIPS*.
* Wang, Y., Wang, W., Joty, S., & Hoi, S. C. (2021). CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. *EMNLP*.
* Wang, Y., Zhou, Y., Wang, W., Zhou, T., Joty, S., & Hoi, S. C. (2022). Code-MVP: Learning to represent source code from multiple views with contrastive pre-training. *arXiv:2205.02029*.

